{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoldste\\Anaconda2\\envs\\py35\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update puncuation list in spacy\n",
    "nlp.vocab[\"$\"].is_punct = True\n",
    "nlp.vocab[\"|\"].is_punct = True\n",
    "nlp.vocab[\"+\"].is_punct = True\n",
    "nlp.vocab[\"<\"].is_punct = True\n",
    "nlp.vocab[\">\"].is_punct = True\n",
    "nlp.vocab[\"=\"].is_punct = True\n",
    "nlp.vocab[\"^\"].is_punct = True\n",
    "nlp.vocab[\"`\"].is_punct = True\n",
    "nlp.vocab[\"~\"].is_punct = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('~/Documents/Capstone/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram = pickle.load(open('trigram2.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = pickle.load(open('lr_model.sav', 'rb'))\n",
    "nb_model = pickle.load(open('tfidf_nb_model.sav', 'rb'))\n",
    "rf_model = pickle.load(open('tfidf_rf_model.sav', 'rb'))\n",
    "svm_model = pickle.load(open('tfidf_svm_model.sav', 'rb'))\n",
    "wv_lr_model = pickle.load(open('wv_lr_model.sav', 'rb'))\n",
    "wv_rf_model = pickle.load(open('wv_rf_model.sav', 'rb'))\n",
    "wv_svm_model = pickle.load(open('wv_svm_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Function to Determine Fake vs. Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in article text\n",
    "with open(\"C:/Users/jgoldste/Documents/Capstone/code/fake_news_test8.txt\", 'r') as f:\n",
    "    article = \"\"\n",
    "    for l in f:\n",
    "        l = l.rstrip()\n",
    "        article += str(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?Police have identified James Alex Fields as the driver of the car that smashed into a crowd of anti-fascist protesters and another vehicle during Charlottesville rallies. Fields faces second degree murder charges for killing a woman and injuring 19 other people. The 20-year-old man was taken into custody following the deadly crash on Saturday afternoon. The suspect has been charged with one count of second degree murder, three counts of malicious wounding and one count of failure to prevent a hit-and-run incident.                Although police have not disclosed any details concerning the prime suspectâ€™s identity, media reported that he was the owner of the grey Dodge Challenger, a vehicle that was filmed by the witnesses speeding up and plowing into the crowd of anti-fascist protesters who flocked to downtown Charlottesville on Saturday to oppose a large far-right rally there.           The guy arrested over the car plowing incident isn't even on the record as owning the Dodge Challenger that injured 20 people and killing one in Charlottesville, VA.                The only name as ownership to the vehicle is Jerome Vangheluwe who is the father of Joel Vangheluwe, who claims to own the Dodge Challenger used in an attack today in Charlottesville Virginia on his personal Facebook page. Also remember police in Charlottesville who made this arrest ordered a stand down so the protest or whatever you want to call it would become violent. However it officially looks like the police have arrested a person not related to the incident as this guys name isn't even attached to the car, so what we basically have is a violent Antifa member running around free in Joel Vangheluwe who is a known Antifa supporter by the way. This James Alex Fields subject is simply being used to make it look like a Right Wing attack, when he has no ties to the Dodge Challenger that was used...Unlike the Vangheluwe family who owns the car.\""
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See article text\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to eliminate tokens that are pure punctuation, whitespace, or stopword\n",
    "# can be updated based on desired filtering \n",
    "\n",
    "def process_txt(token):\n",
    "    return token.is_punct or token.is_space or token.is_stop or token.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to take array of articles and turn them into nested list of tokens\n",
    "\n",
    "def lemmatize_txt(array):\n",
    "    lemma = []\n",
    "    \n",
    "    doc = nlp(array)\n",
    "\n",
    "    lemma.append([n.lemma_ for n in doc if not process_txt(n)])\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to recombine nested list of tokens into full articles \n",
    "\n",
    "def lemma_combine(lis):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    for i in range(len(lis)):\n",
    "        concat_art = ' '.join(lis[i])\n",
    "        parsed_articles.append(concat_art)\n",
    "    \n",
    "    return parsed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_vector(text):\n",
    "    text = text.split()\n",
    "    vector = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            vector.append(model.word_vec(i))\n",
    "        except:\n",
    "            pass\n",
    "    return list(np.mean(vector, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fake_detector(article):\n",
    "    article = re.sub('[^\\x00-\\x7F]+', \"\", article) #remove non-ascii characters\n",
    "    article = re.sub('(\\r)+', \"\",  article) #remove newline characters\n",
    "    article = re.sub(r'@([A-Za-z0-9_]+)', \"\",  article) # remove twitter handles\n",
    "    article = re.sub(r\"(https|http)\\S+\", \"\",  article) # remove hyperlinks\n",
    "\n",
    "    # tokenize article text\n",
    "    lem = lemmatize_txt(article)\n",
    "    \n",
    "    # add text to trigram model\n",
    "    trigram.add_vocab(lem)\n",
    "    trigram_lem = list(trigram[lem])\n",
    "    \n",
    "    # recombine tokens\n",
    "    tri_lem_comb = lemma_combine(trigram_lem)\n",
    "    \n",
    "    # format text for w2v model\n",
    "    tri_lem = pd.Series([x for x in tri_lem_comb])\n",
    "    vec_text = tri_lem.apply(infer_vector)\n",
    "    vec_text = pd.DataFrame(list(map(lambda x: list(x), vec_text)))\n",
    "    \n",
    "    # make predictions\n",
    "    lr_predicted = lr_model.predict(tri_lem_comb)\n",
    "    nb_predicted = nb_model.predict(tri_lem_comb)\n",
    "    rf_predicted = rf_model.predict(tri_lem_comb)\n",
    "    svm_predicted = svm_model.predict(tri_lem_comb)\n",
    "    wv_lr_predicted = wv_lr_model.predict(vec_text)\n",
    "    wv_rf_predicted = wv_rf_model.predict(vec_text)\n",
    "    wv_svm_predicted = wv_svm_model.predict(vec_text)\n",
    "\n",
    "    print (\"TFIDF NB Prediction:\" , lr_predicted)\n",
    "    print (\"TFIDF LR Prediction:\" , nb_predicted)\n",
    "    print (\"TFIDF RF Prediction:\" , rf_predicted)\n",
    "    print (\"TFIDF SVM Prediction:\" , svm_predicted)\n",
    "    print (\"W2V LR Prediction:\" , wv_lr_predicted)\n",
    "    print (\"W2V RF Prediction:\" , wv_rf_predicted)\n",
    "    print (\"W2V SVM Prediction:\" , wv_svm_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoldste\\Anaconda2\\envs\\py35\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF NB Prediction: [0]\n",
      "TFIDF LR Prediction: [0]\n",
      "TFIDF RF Prediction: [0]\n",
      "TFIDF SVM Prediction: [0]\n",
      "W2V LR Prediction: [0]\n",
      "W2V RF Prediction: [0]\n",
      "W2V SVM Prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "fake_detector(article) # predictions for real article about Hurricane Irma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoldste\\Anaconda2\\envs\\py35\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF NB Prediction: [0]\n",
      "TFIDF LR Prediction: [0]\n",
      "TFIDF RF Prediction: [0]\n",
      "TFIDF SVM Prediction: [0]\n",
      "W2V LR Prediction: [0]\n",
      "W2V RF Prediction: [0]\n",
      "W2V SVM Prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "fake_detector(article) # predictions for real article about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoldste\\Anaconda2\\envs\\py35\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF NB Prediction: [1]\n",
      "TFIDF LR Prediction: [1]\n",
      "TFIDF RF Prediction: [1]\n",
      "TFIDF SVM Prediction: [1]\n",
      "W2V LR Prediction: [1]\n",
      "W2V RF Prediction: [0]\n",
      "W2V SVM Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "fake_detector(article) # predictions for fake article about Clinton foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoldste\\Anaconda2\\envs\\py35\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF NB Prediction: [1]\n",
      "TFIDF LR Prediction: [1]\n",
      "TFIDF RF Prediction: [1]\n",
      "TFIDF SVM Prediction: [1]\n",
      "W2V LR Prediction: [1]\n",
      "W2V RF Prediction: [1]\n",
      "W2V SVM Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "fake_detector(article) # predictions for fake article about Seth Rich conspiracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoldste\\Anaconda2\\envs\\py35\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF NB Prediction: [1]\n",
      "TFIDF LR Prediction: [0]\n",
      "TFIDF RF Prediction: [1]\n",
      "TFIDF SVM Prediction: [1]\n",
      "W2V LR Prediction: [1]\n",
      "W2V RF Prediction: [0]\n",
      "W2V SVM Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "fake_detector(article) # predictions for fake article about Charlottesville driver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
